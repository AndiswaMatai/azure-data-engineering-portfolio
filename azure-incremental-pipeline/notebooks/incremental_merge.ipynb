from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("IncrementalPipeline").getOrCreate()

# Load watermark
watermark = "2024-01-01 00:00:00"

# Read new data
df_new = spark.read.parquet("/datalake/bronze/trips") \
    .filter(col("LastUpdatedTimestamp") > watermark)

# Target table
df_target = spark.read.parquet("/datalake/silver/trips")

# Merge logic
df_merged = df_target.union(df_new).dropDuplicates(["TripID"])

df_merged.write.mode("overwrite") \
    .parquet("/datalake/silver/trips")

