# Sample PySpark notebook for mock ETL

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, unix_timestamp, round

# Create Spark session
spark = SparkSession.builder.appName("Mock_ETL").getOrCreate()

# Load raw CSV
df_raw = spark.read.csv("../data/raw/sample_sql_server_table.csv", header=True, inferSchema=True)

# Convert StartTime and EndTime to timestamp
df = df_raw.withColumn("StartTimeTS", to_timestamp(col("TripDate") + " " + col("StartTime"))) \
           .withColumn("EndTimeTS", to_timestamp(col("TripDate") + " " + col("EndTime")))

# Calculate TripDurationMinutes
df = df.withColumn("TripDurationMinutes", round((unix_timestamp("EndTimeTS") - unix_timestamp("StartTimeTS"))/60, 2))

# Calculate RevenuePerHour
df = df.withColumn("RevenuePerHour", round(col("FareCollected") / (col("TripDurationMinutes") / 60), 2))

# Select final columns
df_clean = df.select("DriverID", "RouteID", "TripDate", "TripDurationMinutes", "FareCollected", "RevenuePerHour")

# Save cleaned data to CSV
df_clean.write.csv("../data/clean/sample_clean_table.csv", header=True, mode="overwrite")

df_clean.show()
